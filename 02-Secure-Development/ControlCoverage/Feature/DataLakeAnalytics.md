## IMPORTANT: DevOps Kit (AzSK) is being sunset by end of FY21. More details [here](../../../ReleaseNotes/AzSKSunsetNotice.md)
----------------------------------------------

<html>
<head>

</head><body>
<H2>DataLakeAnalytics</H2><table><tr><th>Description & Rationale</th><th>ControlSeverity</th><th>Automated</th><th>Fix Script</th></tr><tr><td><b>Data Lake Analytics creator must be granted only required Role Based Access Control (RBAC) access on Subscription/Resource Group/Resource</b><br/>Granting minimum access by leveraging RBAC feature ensures that users are granted just enough permissions to perform their tasks. This minimizes exposure of the resources in case of user/service account compromise.</td><td>Medium</td><td>No</td><td>No</td></tr><tr><td><b>All Data Lake Analytics users/service principal must be authenticated using AAD backed credentials</b><br/>Using the native enterprise directory for authentication ensures that there is a built-in high level of assurance in the user identity established for subsequent access control. All Enterprise subscriptions are automatically associated with their enterprise directory (xxx.onmicrosoft.com) and users in the native directory are trusted for authentication to enterprise subscriptions.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>All users/identities must be granted minimum required permissions using Role Based Access Control (RBAC)</b><br/>Granting minimum access by leveraging RBAC feature ensures that users are granted just enough permissions to perform their tasks. This minimizes exposure of the resources in case of user/service account compromise.</td><td>Medium</td><td>Yes</td><td>No</td></tr><tr><td><b>Data Lake Analytics developer (user/service principal) must have least required ACLs on Catalog/Database and Data Lake Store file system</b><br/>Granting minimum permissions to users/security groups on data lake store file system by leveraging Access Control List (ACL) feature ensures that U-SQL jobs executed by users/security groups would perform only intended read/write operations. This minimizes exposure of the data in case of user credentials leak or faulty job programming.</td><td>Medium</td><td>No</td><td>No</td></tr><tr><td><b>Storage account data source must be added securely</b><br/>Setting up storage data source using PowerShell command eliminates the risk of exposing storage account key (because the key itself is not displayed on the GUI at all).</td><td>Medium</td><td>No</td><td>No</td></tr><tr><td><b>Secrets to access SQL Azure/SQL VM/SQL Data Warehouse must be securely stored under Catalog database credentials</b><br/>Credentials added in form of plain text in U-SQL job have higher chances of getting compromised. The job code is also likely check in into a code repository so the credential is visible to many parties.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>U-SQL script file(s) must be uploaded from a secured/trusted location</b><br/>ADLA U-SQL File Import wizard does not impose any restrictions on file format nor does it scan uploaded files for security issues. If these files are at an insecure/untrustworthy location, they can be tampered/infested. This can lead to compromise of the ADLA jobs and data.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>Diagnostics logs must be enabled with a retention period of at least 365 days.</b><br/>Logs should be retained for a long enough period so that activity trail can be recreated when investigations are required in the event of an incident or a compromise. A period of 1 year is typical for several compliance requirements as well.</td><td>Medium</td><td>Yes</td><td>No</td></tr><tr><td><b>Sensitive data must be encrypted at rest</b><br/>Using this feature ensures that sensitive data is stored encrypted at rest. This minimizes the risk of data loss from physical theft and also helps meet regulatory compliance requirements.</td><td>High</td><td>Yes</td><td>No</td></tr><tr><td><b>Sensitive data must be encrypted in transit</b><br/>Use of HTTPS ensures server/service authentication and protects data in transit from network layer man-in-the-middle, eavesdropping, session-hijacking attacks.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>Backup and Disaster Recovery must be planned for the default Data Lake Store account</b><br/>Data Lake Analytics does not offer features to cover backup/disaster recovery out-of-the-box. As a result, when processing critical workloads, a team must have adequate backups of the data and the jobs (catalog, code, etc.).</td><td>Medium</td><td>No</td><td>No</td></tr><tr><td><b>Diagnostic and activity logs for Data Lake Analytics should be reviewed periodically</b><br/>Periodic reviews of diagnostics, activity and audit logs ensures that anomalous activity can be identified early enough instead of after a major compromise.</td><td>Medium</td><td>No</td><td>No</td></tr></table>
<table>
</table>
</body></html>
