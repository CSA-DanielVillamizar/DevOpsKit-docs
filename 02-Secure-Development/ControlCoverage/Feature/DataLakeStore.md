<H2>DataLakeStore</H2><table><tr><th align="left">Description & Rationale</th><th align="left">Severity</th><th align="left">Automated</th><th align="left">Fix Script</th></tr><tr><td><b>All users/applications are authenticated using Azure Active Directory (AAD) based credentials</b><br/>Using the native enterprise directory for authentication ensures that there is a built-in high level of assurance in the user identity established for subsequent access control. All Enterprise subscriptions are automatically associated with their enterprise directory (xxx.onmicrosoft.com) and users in the native directory are trusted for authentication to enterprise subscriptions.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>All users/identities must be granted minimum required permissions using Role Based Access Control (RBAC)</b><br/>Granting minimum access by leveraging RBAC feature ensures that users are granted just enough permissions to perform their tasks. This minimizes exposure of the resources in case of user/service account compromise.</td><td>Medium</td><td>Yes</td><td>No</td></tr><tr><td><b>Access to Data Lake Store file system must be limited by using appropriate Access Control List (ACL). The 'Other' group must not have any access</b><br/>Using appropriate ACLs ensures that data in ADLS is protected and accessible only to the entities with a legitimate need to access it.</td><td>High</td><td>Yes</td><td>No</td></tr><tr><td><b>Firewall should be enabled on Data Lake Store</b><br/>Using the firewall feature ensures that access to the data or the service is restricted to a specific set/group of clients. While this may not be feasible in all scenarios, when it can be used, it provides an extra layer of access control protection for critical assets.</td><td>Medium</td><td>Yes</td><td>No</td></tr><tr><td><b>AdlCopy tool must be used securely while copying data from storage blobs to Data Lake Store</b><br/>Use of HTTPS ensures server/service authentication and protects data in transit from network layer man-in-the-middle, eavesdropping, session-hijacking attacks.ï¿½Incautious use of storage key in AdlCopy command may result into exposure of the key to unauthorized users.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>Clients such as web jobs, standalone apps should use a service principal identity to access Data Lake Store</b><br/>Using a 'user' account should be avoided because, in general, a user account will likely have broader set of privileges to enterprise assets. Using a dedicated SPN ensures that the SPN does not have permissions beyond the ones specifically granted for the given scenario.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>Sensitive data must be encrypted at rest</b><br/>Using this feature ensures that sensitive data is stored encrypted at rest. This minimizes the risk of data loss from physical theft and also helps meet regulatory compliance requirements.</td><td>High</td><td>Yes</td><td>No</td></tr><tr><td><b>Sensitive data must be encrypted in transit</b><br/>Use of HTTPS ensures server/service authentication and protects data in transit from network layer man-in-the-middle, eavesdropping, session-hijacking attacks.</td><td>High</td><td>No</td><td>No</td></tr><tr><td><b>Diagnostics logs must be enabled with a retention period of at least 365 days.</b><br/>Logs should be retained for a long enough period so that activity trail can be recreated when investigations are required in the event of an incident or a compromise. A period of 1 year is typical for several compliance requirements as well.</td><td>Medium</td><td>Yes</td><td>No</td></tr><tr><td><b>Diagnostic logs for Data Lake Store should be reviewed periodically</b><br/>Periodic reviews of diagnostics, activity and audit logs ensures that anomalous activity can be identified early enough instead of after a major compromise.</td><td>Medium</td><td>No</td><td>No</td></tr><tr><td><b>Backup and Disaster Recovery must be planned for Data Lake Store</b><br/>Data Lake Analytics does not offer features to cover backup/disaster recovery out-of-the-box. As a result, when processing critical workloads, a team must have adequate backups of the data.</td><td>Medium</td><td>No</td><td>No</td></tr><tr><td><b>Data in Data Lake Store should be cleaned up using file retention</b><br/>Data should not be retained for periods longer than required for business use case scenarios. Purging/cleaning up data periodically minimizes risk from compromise while also helping limit the costs of maintaining it.</td><td>Medium</td><td>No</td><td>No</td></tr></table>
